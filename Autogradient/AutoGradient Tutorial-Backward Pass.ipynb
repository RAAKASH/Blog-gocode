{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    \"\"\"\n",
    "    Refer The below link to know why mutable datasetrutures cant be in the initialization\n",
    "    https://stackoverflow.com/questions/4841782/python-constructor-and-default-value\n",
    "    \"\"\"\n",
    "    def __init__(self,value,parent=None,child=None,operation=None,grad=None,gradstatus =0):\n",
    "        self.value = value\n",
    "        self.grad = grad\n",
    "        self.operation = operation\n",
    "        self.gradstatus = 0\n",
    "        if child is None:\n",
    "            self.child = []\n",
    "        else:\n",
    "            self.child = child\n",
    "        if parent is None:\n",
    "            self.parent = []\n",
    "        else:\n",
    "            self.parent = parent\n",
    "            \n",
    "        \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    def get_consumer(self):\n",
    "        return self.child\n",
    "    def get_parent(self):\n",
    "        return self.parent\n",
    "    def get_operation(self):\n",
    "        return self.operation\n",
    "    def get_grad(self):\n",
    "        return self.grad\n",
    "    def get_gradstatus(self):\n",
    "        return self.gradstatus\n",
    "    \n",
    "    def update_child(self,child):\n",
    "        self.child.append(child)\n",
    "    def update_value(self,value):\n",
    "        self.value = value\n",
    "    def update_grad(self,grad):\n",
    "        self.grad += grad\n",
    "   \n",
    "    def set_grad(self,grad):\n",
    "        self.grad = grad\n",
    "    def set_gradstatus(self,stat):\n",
    "        self.gradstatus = stat\n",
    "        \n",
    "    def size(self):\n",
    "        return 1\n",
    "    def __repr__(self):\n",
    "        return 'tensor object:'+ str(id(self)) \n",
    "\n",
    "class operation:\n",
    "    def mult(a,b,bprop=False,grad = None):\n",
    "        a_val = a.get_value()\n",
    "        b_val = b.get_value()\n",
    "        \n",
    "        if bprop == False:\n",
    "            res  =  tensor(value = a_val*b_val,parent =[a,b],operation=operation.mult)\n",
    "            a.update_child(res)\n",
    "            b.update_child(res)\n",
    "         \n",
    "        else:\n",
    "            res = (b_val*grad,a_val*grad)\n",
    "          \n",
    "        \n",
    "        return res\n",
    "\n",
    "    def add(a,b,bprop=False,grad = None):\n",
    "        a_val = a.get_value()\n",
    "        b_val = b.get_value()\n",
    "        \n",
    "        if bprop == False:\n",
    "            res  =  tensor(value = a_val+b_val,parent = [a,b],operation=operation.add)\n",
    "            a.update_child(res)\n",
    "            b.update_child(res)\n",
    "        else:\n",
    "            res = (1*grad,1*grad)\n",
    "          \n",
    "        return res\n",
    "    \n",
    "    def sub(a,b,bprop=False,grad = None):\n",
    "        a_val = a.get_value()\n",
    "        b_val = b.get_value()\n",
    "        \n",
    "        if bprop == False:\n",
    "            res  =  tensor(value = a_val-b_val,parent =[a,b],operation=operation.sub)\n",
    "            a.update_child(res)\n",
    "            b.update_child(res)\n",
    "        else:\n",
    "            res = (1*grad,-1*grad)\n",
    "          \n",
    "        return res\n",
    "    \n",
    "    def div(a,b,bprop=False,grad = None):\n",
    "        a_val = a.get_value()\n",
    "        b_val = b.get_value()\n",
    "        \n",
    "        if bprop == False:\n",
    "            res  =  tensor(value = a_val/b_val,parent =[a,b],operation=operation.div)\n",
    "            a.update_child(res)\n",
    "            b.update_child(res)\n",
    "        else:\n",
    "            res = (1/b_val*grad,-a_val/b_val**2*grad)\n",
    "          \n",
    "        return res\n",
    "  \n",
    "\n",
    "class graph:\n",
    "    def __init__(self,func,var=None):\n",
    "        self.func = func\n",
    "        if var is None:\n",
    "            self.var = []\n",
    "        else:\n",
    "            self.var = var\n",
    "        self.func_anc = []\n",
    "        self.var_desc = []\n",
    "        self.gr = [] #final graph\n",
    "        self.tmp = [] # stores cache nodes of desc and anc\n",
    "    \n",
    "    def get_ancestors(self,var):\n",
    "        self.tmp=[]\n",
    "        self.__get_ancestors__(var)\n",
    "        return self.tmp\n",
    "    \n",
    "    def get_descendents(self,var):\n",
    "        self.tmp=[]\n",
    "        self.__get_descendents__(var)\n",
    "        return self.tmp\n",
    "    \n",
    "    def get_graph(self):\n",
    "        if len(self.gr) is 0:\n",
    "            self.__get_graph__()\n",
    "        return self.gr\n",
    "    \n",
    "    def get_grad(self):\n",
    "        self.__get_graph__()   \n",
    "        self.__grad_init__()\n",
    "        self.__get_grad__(self.var)\n",
    "        grad_table =[]\n",
    "        for v in self.var:\n",
    "            grad_table += [v.get_grad()]\n",
    "        return grad_table\n",
    "        \n",
    "    \n",
    "    def __get_grad__(self,var):\n",
    "        for v in var:\n",
    "            if v.get_gradstatus() is 0:\n",
    "                child = self.__get_consumer__(v)\n",
    "                #if child.size:\n",
    "                for ch in child:\n",
    "                    op = ch.get_operation()\n",
    "                    par =  self.__get_parent__(ch)\n",
    "                    pa1,pa2 = ch.get_parent()\n",
    "                    self.__get_grad__([ch])\n",
    "                    pa1_grad_temp,pa2_grad_temp = op(pa1,pa2,True,ch.get_grad())\n",
    "                    if pa1 == v:\n",
    "                        v.update_grad(pa1_grad_temp)\n",
    "                    elif pa2 == v:\n",
    "                        v.update_grad(pa2_grad_temp)\n",
    "\n",
    "            v.set_gradstatus(1)            \n",
    "                    \n",
    "    \n",
    "    def __get_graph__(self):\n",
    "        if len(self.gr) is 0:\n",
    "            self.__var_union__()\n",
    "            self.__func_anc__()\n",
    "            self.gr = list(set(self.func_anc)&set(self.var_desc)|set(self.var)|set([self.func]))\n",
    "        \n",
    "        \n",
    "    def __grad_init__(self):\n",
    "        for nodes in self.gr:\n",
    "            nodes.set_grad(0)\n",
    "            nodes.set_gradstatus(0)\n",
    "        self.func.set_grad(1)\n",
    "        self.func.set_gradstatus(1)\n",
    "\n",
    "                    \n",
    "    \n",
    "    def __get_ancestors__(self,var):\n",
    "        parent = var.get_parent()\n",
    "        for p in parent:\n",
    "            self.tmp.append(p)\n",
    "            if p.size :\n",
    "                self.__get_ancestors__(p)\n",
    "               \n",
    "    \n",
    "    def __get_descendents__(self,var):\n",
    "        child = var.get_consumer()\n",
    "        for ch in child:\n",
    "            self.tmp.append(ch)\n",
    "            if ch.size:\n",
    "                self.__get_descendents__(ch)\n",
    "        \n",
    "    \n",
    "    def __var_union__(self):\n",
    "        for v in self.var:\n",
    "            self.var_desc = list(set(self.var_desc)|set(self.get_descendents(v)))\n",
    "   \n",
    "    def __func_anc__(self):\n",
    "        self.func_anc = self.get_ancestors(self.func)\n",
    "        \n",
    "    def __get_consumer__(self,v):\n",
    "        tmp_res = v.get_consumer()\n",
    "        return (list(set(tmp_res)&set(self.gr)))\n",
    "    \n",
    "    def __get_parent__(self,v):\n",
    "        tmp_res = v.get_parent()\n",
    "        return (list(set(tmp_res)&set(self.gr)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "Derived through Bakpropagation : [0.06818181818181818, 0.05681818181818182]\n",
      "Actual gradients : 0.06818181818181818 0.056818181818181816\n",
      "\n",
      "\n",
      "Test 2\n",
      "Derived through Bakpropagation : [-0.5866666666666667, -0.48888888888888893]\n",
      "Actual gradients : -0.5866666666666667 -0.48888888888888893\n"
     ]
    }
   ],
   "source": [
    "# Testing our backpropagation code\n",
    "op = operation # Math operation class\n",
    "\n",
    "# Input initialzation\n",
    "a = tensor(value = 5)\n",
    "b = tensor(value = 6)\n",
    "c = tensor(value = 8)\n",
    "d = tensor(value = 11)\n",
    "# Performing few operation\n",
    "e = op.mult(a,b)\n",
    "f = op.mult(c,d)\n",
    "g = op.div(e,f)#\n",
    "h = op.div(tensor(1),g)\n",
    "\n",
    "gr = graph(func = g,var = [a,b])\n",
    "\n",
    "# gr.get_graph() #- Gives you the pruned graph\n",
    "\n",
    "# Getting gradients of g wrt (a and b)\n",
    "print('Test 1')\n",
    "print('Derived through Bakpropagation :',gr.get_grad())\n",
    "print('Actual gradients :',6/8/11,5/8/11)\n",
    "print('\\n')\n",
    "# Getting gradients of h wrt (a and b)\n",
    "print('Test 2')\n",
    "gr = graph(func = h,var = [a,b])\n",
    "gr.get_graph()\n",
    "print('Derived through Bakpropagation :',gr.get_grad())\n",
    "print('Actual gradients :',-(6/8/11)/(30/88)**2,-(5/8/11)/(30/88)**2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
